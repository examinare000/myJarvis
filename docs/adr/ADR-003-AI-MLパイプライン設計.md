# ADR-003: AI/MLパイプライン設計

**ステータス:** 承認済み
**日付:** 2025-09-25
**意思決定者:** 開発チーム

## コンテキスト

myJarvisプロジェクトは「タスク分解のためのLLM」と「スケジューリング最適化のためのローカルML」という2つの異なるAI/ML機能を要求しています。プライバシー、コスト、パフォーマンスを考慮し、これらの機能をどのように実装・連携させるかを決定する必要があります。

## 要求事項
- タスク分解処理（LLM使用）
- ユーザーの満足度、休憩時間等のデータ収集
- ローカルMLによるスケジューリング最適化
- プライバシー保護
- リアルタイム性の確保

## 検討した選択肢

### 1. フルクラウド実行
- **概要**: すべてのデータをクラウドに送信し、LLM APIとクラウド上のMLモデルで全ての処理を実行
- **長所**:
  - 強力なモデルを利用可能
  - 実装が比較的容易
  - スケーラビリティの確保
- **短所**:
  - ユーザーの全データを外部に送信、プライバシー懸念が非常に大きい
  - API利用料やサーバーコストの継続的発生
  - ネットワーク依存による可用性リスク

### 2. フルローカル実行
- **概要**: LLMもMLモデルも全てデバイス上で実行
- **長所**:
  - プライバシーが最大限保護
  - オフラインでも動作可能
  - 運用コストの削減
- **短所**:
  - デバイスの計算資源を大きく消費
  - 高性能なLLMをデバイス上で動かすのは現実的ではない
  - モデルのアップデート・管理が困難

### 3. ハイブリッド実行
- **概要**: 処理の特性に応じてクラウドとローカルを使い分け
- **長所**:
  - 両者の利点を両立可能
  - 柔軟なアーキテクチャ
- **短所**:
  - アーキテクチャが複雑化
  - データ分離の管理が必要

## 決定

**ハイブリッド実行アプローチを採用します。**

- **タスク分解 (LLM)**: サーバーサイド（BFF）から外部の高性能LLM API（Google Gemini API, OpenAI API）を呼び出し
- **スケジューリング最適化 (ML)**: クライアントサイド（デバイス上）で実行、Kotlin Multiplatformの共有コアに実装

### 決定理由

1. **LLMの能力活用**: タスク分解のような高度な自然言語処理は、クラウド上の大規模モデルを利用するのが最も効果的
2. **プライバシー保護**: スケジュール、満足度、バイタルといった機微な情報はデバイス内に留め、ローカルMLで処理
3. **要件との一致**: 「ローカルMLによる最適化」という技術要件に明確に合致

## アーキテクチャ設計

### データフローとMLパイプライン

```
[External Data Sources]
    ↓ (via BFF)
[LLM Processing (Server)]
    ↓ (Task Structure)
[Client Application]
    ↓ (User Behavior Data)
[Local ML Model (KMP Shared Core)]
    ↓ (Scheduling Optimization)
[User Interface (iOS/Web)]
```

### LLMパイプライン（サーバーサイド）
1. **データ前処理**: 外部サービスからの生データをクリーニング・正規化
2. **プライバシー保護**: 個人特定情報のマスキング・匿名化
3. **プロンプト生成**: タスク分解に最適化されたプロンプトを動的生成
4. **LLM API呼び出し**: Gemini API / OpenAI APIへのリクエスト
5. **レスポンス処理**: 構造化されたタスクデータとしてクライアントに返却

### ローカルMLパイプライン（クライアントサイド）
1. **特徴量生成**: ユーザーの行動パターン、満足度、時間帯等から特徴量を抽出
2. **モデル推論**: 軽量なMLモデル（TensorFlow Lite / Core ML）でスケジューリング最適化
3. **結果統合**: LLMによるタスク分解結果と組み合わせて最終的なスケジュールを生成
4. **継続学習**: ユーザーのフィードバックによるモデルの逐次更新

## 技術選定

### LLM関連技術
- **API**: Google Gemini 2.5 Pro / OpenAI GPT-4
- **言語**: Node.js + TypeScript（BFF内）
- **プロンプト管理**: テンプレート化されたプロンプト管理システム
- **キャッシング**: Redis による API レスポンスキャッシュ

### ローカルML関連技術
- **フレームワーク**:
  - iOS: Core ML
  - Web: TensorFlow.js
  - KMP共有: TensorFlow Lite (via JNI/WASM)
- **モデル形式**: TensorFlow Lite (.tflite)
- **学習手法**: オンデバイス連合学習（Federated Learning）

## 影響と結果

### ポジティブな影響
- 高度なタスク分解能力の確保
- 機微な情報のプライバシー保護
- ユーザーごとのパーソナライゼーション
- オフライン時の基本機能継続

### ネガティブな影響とリスク軽減策
- **アーキテクチャ複雑性**: 明確なインターフェース設計とドキュメント化
- **LLM API コスト**: 効率的なプロンプト設計とキャッシング戦略
- **モデル管理**: 自動化されたモデル配信・更新システムの構築

### データプライバシー対策
- LLM処理時の個人情報マスキング
- ローカルML学習データの暗号化
- モデル更新時の差分プライバシー適用
- ユーザーの明示的な同意取得

## 関連する意思決定
- ADR-001: マルチプラットフォーム戦略（KMP共有コアでのML実行）
- ADR-002: 外部サービス統合戦略（BFF経由のLLM処理）
- ADR-005: データストレージ・プライバシー戦略（機微データのローカル保持）